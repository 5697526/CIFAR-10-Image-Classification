{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b372e1",
   "metadata": {},
   "source": [
    "李佩瑶 22300180089 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d13c95",
   "metadata": {},
   "source": [
    "## 一、实验目的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7cb338",
   "metadata": {},
   "source": [
    "使用 numpy 手工搭建三层神经网络分类器，在数据集 CIFAR-10 上进行训练以实现图像分类。至少包含模型、训练、测试和参数查找四个部分。其中模型部分应允许自定义隐藏层大小、激活函数类型，支持通过反向传播计算给定损失的梯度；训练部分应实现 SGD 优化器、学习率下降、交叉熵损失和 L2 正则化，并能根据验证集指标自动保存最优的模型权重；参数查找环节要求调节学习率、隐藏层大小、正则化强度等超参数，观察并记录模型在不同超参数下的性能；测试部分需支持导入训练好的模型，输出在测试集上的分类准确率（Accuracy）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8fd892",
   "metadata": {},
   "source": [
    "## 二、数据集介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5879378",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "### 1. 数据集来源  \n",
    "\n",
    "\n",
    "本次实验使用的是 CIFAR - 10 数据集，该数据集可从 [CIFAR - 10 ](https://www.cs.toronto.edu/~kriz/cifar.html)下载。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0a6c55",
   "metadata": {},
   "source": [
    "### 2. 数据集内容\n",
    "\n",
    "CIFAR - 10 数据集总共包含 60,000 张 32×32 像素的彩色图像，涵盖有10个不同的类别：飞机（Airplane），汽车（Automobile），鸟（Bird），猫（Cat），鹿（Deer），狗（Dog），青蛙（Frog），马（Horse），船（Ship）和卡车（Truck）。数据集被划分为训练集和测试集两部分，其中训练集包含 50,000 张图像，每个类别有 5,000 张，测试集包含 10,000 张图像，每个类别有 1,000 张。图像采用 RGB 三通道模式，每个通道是一个 32×32 的矩阵，矩阵中的每个元素表示该通道在对应像素位置的颜色强度，取值范围为 0 - 255。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f78d9dd",
   "metadata": {},
   "source": [
    "### 3. 数据格式与存储\n",
    "\n",
    "CIFAR - 10 数据集以二进制文件的形式存储，分为多个批次。训练集被分成 5 个数据批次（data_batch_1 - data_batch_5），每个批次包含 10,000 张图像。测试集则存储在一个单独的文件（test_batch）中。每个数据批次文件包含图像数据和对应的标签，图像数据以一维数组的形式存储。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73258c2",
   "metadata": {},
   "source": [
    "## 三、实验过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59320e9",
   "metadata": {},
   "source": [
    "### 1. 实验准备\n",
    "\n",
    "确保已经安装了 Python 环境，并且安装了项目所需的依赖库。导入所需的库。\n",
    "\n",
    "将 CIFAR - 10 数据集下载并解压到 ./data 目录下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dc01b0",
   "metadata": {},
   "source": [
    "### 2. 数据处理\n",
    "\n",
    "data_loader.py：预处理数据，对下载好的 CIFAR - 10 数据集进行数据加载、归一化和划分验证集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3a48d",
   "metadata": {},
   "source": [
    "#### (1) 数据加载：   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba47807",
   "metadata": {},
   "source": [
    "load_data函数用于加载 CIFAR-10 数据集。通过循环遍历data_batch_1到data_batch_5文件，使用pickle库读取训练数据，再读取test_batch文件中的测试数据。将每个文件中的'data'部分添加到train_data列表中，将'labels'部分扩展到train_labels列表中。对数据也进行类型转换和归一化操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31049faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir='./data'):\n",
    "    train_data, train_labels = [], []\n",
    "    for i in range(1, 6):\n",
    "        with open(os.path.join(data_dir, f'cifar-10-batches-py/data_batch_{i}'), 'rb') as f:\n",
    "            batch = pickle.load(f, encoding='latin1')\n",
    "            train_data.append(batch['data'])\n",
    "            train_labels.extend(batch['labels'])\n",
    "\n",
    "    with open(os.path.join(data_dir, 'cifar-10-batches-py/test_batch'), 'rb') as f:\n",
    "        batch = pickle.load(f, encoding='latin1')\n",
    "        test_data = batch['data']\n",
    "        test_labels = batch['labels']\n",
    "\n",
    "    train_data = np.vstack(train_data).astype(np.float32) / 255.0\n",
    "    test_data = np.array(test_data, dtype=np.float32) / 255.0\n",
    "\n",
    "    return train_data, np.array(train_labels), test_data, np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f62e13",
   "metadata": {},
   "source": [
    "#### (2) 数据预处理  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05304a75",
   "metadata": {},
   "source": [
    "preprocess_data函数用于对训练数据和测试数据进行标准化处理。通过np.mean和np.std函数计算训练数据在各个维度上的均值和标准差，对训练数据和测试数据进行标准化操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d845b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data):\n",
    "    mean = np.mean(train_data, axis=0)\n",
    "    std = np.std(train_data, axis=0)\n",
    "    train_data = (train_data-mean) / (std+1e-7)\n",
    "    test_data = (test_data-mean) / (std+1e-7)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d34f24a",
   "metadata": {},
   "source": [
    "#### (3) 划分训练集和验证集 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10751576",
   "metadata": {},
   "source": [
    "validation函数用于从训练数据中划分出验证集。使用np.random.permutation函数对训练数据的索引进行随机打乱,将打乱后的索引划分为验证集索引val_idx和训练集索引train_idx。根据划分好的索引，从训练数据和训练标签中提取出验证集和新的训练集数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8044aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(train_data, train_labels, val_ratio=0.1):\n",
    "    num_val = int(val_ratio*train_data.shape[0])\n",
    "    indices = np.random.permutation(train_data.shape[0])\n",
    "    val_idx, train_idx = indices[:num_val], indices[num_val:]\n",
    "\n",
    "    return train_data[train_idx], train_labels[train_idx], train_data[val_idx], train_labels[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a11e08",
   "metadata": {},
   "source": [
    "### 3. 神经网络模型构建\n",
    "\n",
    "model.py：使用 numpy 手工搭建三层神经网络分类器，定义了 ThreeLayerNN 类，包含前向传播、反向传播、损失计算、预测和准确率计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b68c80",
   "metadata": {},
   "source": [
    "#### (1) 初始化模型  \n",
    "\n",
    "根据传入的输入大小、隐藏层大小、输出大小和选择的激活函数，加入随机值初始化权重矩阵w1、w2、w3，将偏置向量b1、b2、b3初始化为零向量，并定义四种可选的激活函数操作：标准ReLU，Leaky ReLU，Sigmoid 和 Tanh。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450b97f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation):\n",
    "        self.w1 = np.random.randn(\n",
    "            input_size, hidden_size) * np.sqrt(2/input_size)\n",
    "        self.w2 = np.random.randn(\n",
    "            hidden_size, hidden_size) * np.sqrt(2/hidden_size)\n",
    "        self.w3 = np.random.randn(\n",
    "            hidden_size, output_size) * np.sqrt(2/hidden_size)\n",
    "\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.b2 = np.zeros((1, hidden_size))\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "\n",
    "        self.activation = activation\n",
    "        if activation == 'relu':\n",
    "            self.activation_fn = lambda x: np.maximum(0, x)\n",
    "            self.activation_deriv = lambda x: (x > 0).astype(float)\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation_fn = lambda x: 1 / (1 + np.exp(-x))\n",
    "            self.activation_deriv = lambda x: self.activation_fn(\n",
    "                x) * (1 - self.activation_fn(x))\n",
    "        elif activation == 'tanh':\n",
    "            self.activation_fn = lambda x: np.tanh(x)\n",
    "            self.activation_deriv = lambda x: 1 - np.tanh(x)**2\n",
    "        elif activation == \"leaky_relu\":\n",
    "            self.activation_fn = lambda x: np.where(x > 0, x, 0.01*x)\n",
    "            self.activation_deriv = lambda x: np.where(x > 0, 1, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd5b9c2",
   "metadata": {},
   "source": [
    "#### (2) 前向传播\n",
    "\n",
    "在训练过程中，通过forward方法进行前向传播。输入数据x依次经过与权重矩阵的乘法运算和偏置向量的加法运算，再经过激活函数处理，得到最终的预测概率probs。\n",
    "\n",
    "第一隐藏层计算$z_1 = x w_1+b_1$的线性组合，再通过激活函数进行非线性变换得到$a_1 = f(z_1)$；第二隐藏层同样计算$z_2 = a_1 w_2+b_2$的线性组合，再通过激活函数进行非线性变换得到$a_2 = f(z_2)$；输出层计算$z_3 = a_2 w_3+b_3$的线性组合，将输出转换为概率分布返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7631b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    self.z1 = np.dot(x, self.w1) + self.b1\n",
    "    self.a1 = self.activation_fn(self.z1)\n",
    "\n",
    "    self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "    self.a2 = self.activation_fn(self.z2)\n",
    "\n",
    "    self.z3 = np.dot(self.a2, self.w3) + self.b3\n",
    "    scores = np.exp(self.z3 - np.max(self.z3, axis=1, keepdims=True))\n",
    "    self.probs = scores / np.sum(scores, axis=1, keepdims=True)\n",
    "\n",
    "    return self.probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f3504",
   "metadata": {},
   "source": [
    "#### (3) 反向传播\n",
    "\n",
    "backward方法进行反向传播，计算损失函数关于权重和偏置的梯度，以便使用梯度下降法更新参数。输出层计算误差$d_3 = probs - y$，再计算权重和偏置的梯度$dw_3 = {a_2}^{T}d_3 + \\lambda w_3$和$db_3 = \\sum(d_3, \\text{axis}=0)$；第二层隐藏层计算误差$d_2 = d_3 W_3^T \\odot f'(z_2)$，计算权重和偏置的梯度$dw_2 = a_1^T d_2 + \\lambda W_2$，$db_2 = \\sum(d_2和 \\text{axis}=0)$；第一层隐藏层类似地计算误差$d_1 = d_2 W_2^T \\odot f'(z_1)$，计算权重和偏置的梯度$dw_1 = X^T d_1 + \\lambda W_1$和$db_1 = \\sum(d_1, \\text{axis}=0)$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c95b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, x, y, reg_lambda):\n",
    "    n = x.shape[0]\n",
    "    d3 = self.probs\n",
    "    d3[range(n), y] -= 1\n",
    "    d3 /= n\n",
    "\n",
    "    dw3 = np.dot(self.a2.T, d3) + reg_lambda * self.w3\n",
    "    db3 = np.sum(d3, axis=0, keepdims=True)\n",
    "    d2 = np.dot(d3, self.w3.T) * self.activation_deriv(self.z2)\n",
    "\n",
    "    dw2 = np.dot(self.a1.T, d2) + reg_lambda * self.w2\n",
    "    db2 = np.sum(d2, axis=0, keepdims=True)\n",
    "    d1 = np.dot(d2, self.w2.T) * self.activation_deriv(self.z1)\n",
    "\n",
    "    dw1 = np.dot(x.T, d1) + reg_lambda * self.w1\n",
    "    db1 = np.sum(d1, axis=0, keepdims=True)\n",
    "\n",
    "    return dw1, db1, dw2, db2, dw3, db3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87586f1",
   "metadata": {},
   "source": [
    "#### (4) 损失函数\n",
    "\n",
    "loss_fun函数采用交叉熵损失函数，并加入 L2 正则化项，应用公式$L = \\frac{1}{N}\\sum_{i=1}^{N}-\\log(\\text{probs}_{i,y_i})+\\frac{1}{2}\\lambda(\\sum_{j}W_{1j}^2+\\sum_{j}W_{2j}^2+\\sum_{j}W_{3j}^2)$。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18213b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun(self, x, y, reg_lambda):\n",
    "    n = x.shape[0]\n",
    "    probs = self.forward(x)\n",
    "    n_log_probs = -np.log(probs[range(n), y])\n",
    "    loss = np.sum(n_log_probs) / n + 0.5 * reg_lambda * (np.sum(self.w1 **\n",
    "                                                                    2) + np.sum(self.w2**2) + np.sum(self.w3**2))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7378073",
   "metadata": {},
   "source": [
    "#### (5) 模型评估\n",
    "\n",
    "predict函数对测试集数据进行预测。该方法通过前向传播得到预测概率，然后选择概率最大的类别作为预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a3f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    probs = self.forward(x)\n",
    "    return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbef35d",
   "metadata": {},
   "source": [
    "accuracy函数计算模型在测试集上的准确率。通过比较预测结果与真实标签，统计正确预测的样本数量占总样本数量的比例，得到模型的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e70dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(self, x, y):\n",
    "    preds = self.predict(x)\n",
    "    return np.mean(preds == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704c8654",
   "metadata": {},
   "source": [
    "### 4. 模型训练\n",
    "\n",
    "train.py：实现了三层神经网络模型的训练过程，采用随机梯度下降优化算法和学习率衰减策略，同时记录训练过程中的关键信息，方便后续分析和评估模型性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c934bcb0",
   "metadata": {},
   "source": [
    "#### (1) 训练过程  \n",
    "\n",
    "\n",
    "training函数先初始化模型，依据训练数据的特征维度确定输入层大小 input_size，通过训练标签的唯一类别数确定输出层大小 output_size。实例化 ThreeLayerNN 模型，传入输入层大小、隐藏层大小、输出层大小和激活函数类型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b5627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_data, train_labels, val_data, val_labels, config):\n",
    "    input_size = train_data.shape[1]\n",
    "    output_size = len(np.unique(train_labels))\n",
    "\n",
    "    model = ThreeLayerNN(\n",
    "        input_size, config['hidden_size'], output_size, config['activation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a095a76d",
   "metadata": {},
   "source": [
    "\n",
    "从 config 字典获取训练所需的超参数，如训练轮数、批量大小、学习率、正则化系数和学习率衰减率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be469d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    num_epochs = config['num_epochs']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    reg_lambda = config['reg_lambda']\n",
    "    learning_rate_decay = config['learning_rate_decay']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add9cb1",
   "metadata": {},
   "source": [
    "创建 history 字典记录训练损失、验证损失、验证准确率、最佳准确率和最佳权重。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb34a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'accuracy': [],\n",
    "        'best_accuracy': 0.0,\n",
    "        'best_weights': None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd29252",
   "metadata": {},
   "source": [
    "\n",
    "用循环训练模型，加入学习率衰减，即每 10 个训练轮次后，学习率乘以衰减率。每轮训练前随机打乱训练数据，避免模型学习到数据的顺序。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58019a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    n = train_data.shape[0]\n",
    "    iterations = max(n // batch_size, 1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch % 10 == 0 and epoch > 0:\n",
    "            learning_rate *= learning_rate_decay\n",
    "\n",
    "        indices = np.random.permutation(n)\n",
    "        train_data = train_data[indices]\n",
    "        train_labels = train_labels[indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13b0b22",
   "metadata": {},
   "source": [
    "\n",
    "将训练数据分成多个批量，依次进行前向传播、反向传播和参数更新。每轮训练结束后，计算训练集和验证集的损失以及验证集的准确率，并记录到 history 中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ed128",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        for i in range(iterations):\n",
    "            start_i = i * batch_size\n",
    "            end_i = min((i+1) * batch_size, n)\n",
    "            batch_data = train_data[start_i:end_i]\n",
    "            batch_labels = train_labels[start_i:end_i]\n",
    "\n",
    "            model.forward(batch_data)\n",
    "            dw1, db1, dw2, db2, dw3, db3 = model.backward(\n",
    "                batch_data, batch_labels, reg_lambda)\n",
    "\n",
    "            model.w1 -= learning_rate * dw1\n",
    "            model.b1 -= learning_rate * db1\n",
    "            model.w2 -= learning_rate * dw2\n",
    "            model.b2 -= learning_rate * db2\n",
    "            model.w3 -= learning_rate * dw3\n",
    "            model.b3 -= learning_rate * db3\n",
    "\n",
    "        train_loss = model.loss_fun(train_data, train_labels, reg_lambda)\n",
    "        val_loss = model.loss_fun(val_data, val_labels, reg_lambda)\n",
    "        accuracy = model.accuracy(val_data, val_labels)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7991fc25",
   "metadata": {},
   "source": [
    "\n",
    "若当前验证集准确率高于之前的最佳准确率，则更新最佳准确率和最佳权重。\n",
    "每 10 个训练轮次输出一次训练损失、验证损失和验证准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b719473",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        if accuracy > history['best_accuracy']:\n",
    "            history['best_accuracy'] = accuracy\n",
    "            history['best_weights'] = {\n",
    "                'w1': model.w1.copy(),\n",
    "                'b1': model.b1.copy(),\n",
    "                'w2': model.w2.copy(),\n",
    "                'b2': model.b2.copy(),\n",
    "                'w3': model.w3.copy(),\n",
    "                'b3': model.b3.copy()\n",
    "            }\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{num_epochs}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, accuracy={accuracy:.4f}\")\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2104ab",
   "metadata": {},
   "source": [
    "#### (2) 绘制训练曲线\n",
    "\n",
    "plot_training函数绘制训练过程中的损失曲线和准确率曲线。将绘制的图像保存到 save_path 路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7991399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(history, save_path=None):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Validation Accuracy')\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dadc82",
   "metadata": {},
   "source": [
    "#### (3)保存模型\n",
    "\n",
    "save_model函数把模型的权重和激活函数类型保存到指定路径的 .npy 文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732003d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    np.save(path, {\n",
    "        'w1': model.w1,\n",
    "        'b1': model.b1,\n",
    "        'w2': model.w2,\n",
    "        'b2': model.b2,\n",
    "        'w3': model.w3,\n",
    "        'b3': model.b3,\n",
    "        'activation': model.activation\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e1bc81",
   "metadata": {},
   "source": [
    "#### (4)加载模型\n",
    "\n",
    "load_model函数从指定路径的 .npy 文件中加载模型的权重和激活函数类型，并返回一个初始化好的 ThreeLayerNN 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ce099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path, input_size, hidden_size, output_size):\n",
    "    data = np.load(path, allow_pickle=True).item()\n",
    "    model = ThreeLayerNN(input_size, hidden_size,\n",
    "                         output_size, data['activation'])\n",
    "    model.w1 = data['w1']\n",
    "    model.b1 = data['b1']\n",
    "    model.w2 = data['w2']\n",
    "    model.b2 = data['b2']\n",
    "    model.w3 = data['w3']\n",
    "    model.b3 = data['b3']\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7581a037",
   "metadata": {},
   "source": [
    "### 5. 超参数搜索\n",
    "\n",
    "hyperparam_tuning.py：进行超参数调优，通过网格搜索的方法，在给定的超参数组合范围内，寻找能使模型在验证集上表现最优的超参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c049e2",
   "metadata": {},
   "source": [
    "#### (1) 超参数调优\n",
    "\n",
    "hyperparameter()函数进行超参数调优。首先定义超参数搜索空间，涵盖隐藏层大小、学习率、正则化系数、激活函数和学习率衰减率，同时设定了固定的训练轮数和批量大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a8880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter():\n",
    "    hidden_sizes = [512]\n",
    "    learning_rates = [0.018, 0.019, 0.017]\n",
    "    reg_lambdas = [0.011]\n",
    "    activations = ['leaky_relu']\n",
    "    learning_rate_decays = [0.98]\n",
    "\n",
    "    num_epochs = 20\n",
    "    batch_size = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c856b0",
   "metadata": {},
   "source": [
    "加载和预处理数据，将训练数据划分为训练集和验证集，如果在数据加载或预处理过程中出现异常，打印错误信息并返回空列表和空字典。初始化结果列表和总组合数。打印开始超参数搜索的信息，显示总共需要测试的组合数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc124be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    try:\n",
    "        train_data, train_labels, test_data, test_labels = load_data()\n",
    "        train_data, test_data = preprocess_data(train_data, test_data)\n",
    "        train_data, train_labels, val_data, val_labels = validation(\n",
    "            train_data, train_labels)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return [], {}\n",
    "\n",
    "    results = []\n",
    "    total_combinations = len(hidden_sizes) * len(learning_rates) * \\\n",
    "        len(reg_lambdas) * len(activations) * len(learning_rate_decays)\n",
    "    current = 0\n",
    "\n",
    "    print(\n",
    "        f\"\\nStarting hyperparameter search ({total_combinations} combinations)...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3911a3a7",
   "metadata": {},
   "source": [
    "遍历超参数组合，构建包含当前超参数组合的字典config，将其传递给 training 函数进行模型训练。如果在训练过程中出现异常，打印错误信息并跳过当前组合，继续测试下一个组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b610bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for hidden_size, lr, reg, activation, decay in itertools.product(\n",
    "            hidden_sizes, learning_rates, reg_lambdas, activations, learning_rate_decays):\n",
    "\n",
    "        current += 1\n",
    "        print(f\"\\n[{current}/{total_combinations}] Testing: \"\n",
    "              f\"hidden_size={hidden_size}, lr={lr:.3f}, reg={reg:.3f}, activation={activation}, decay={decay}\")\n",
    "\n",
    "        config = {\n",
    "            'hidden_size': hidden_size,\n",
    "            'activation': activation,\n",
    "            'num_epochs': num_epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'learning_rate': lr,\n",
    "            'reg_lambda': reg,\n",
    "            'learning_rate_decay': decay\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            _, history = training(\n",
    "                train_data, train_labels, val_data, val_labels, config)\n",
    "        except Exception as e:\n",
    "            print(f\"Error training model with hyperparameters {config}: {e}\")\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cba36d",
   "metadata": {},
   "source": [
    "从训练历史记录中找出验证集上的最佳准确率best_val_acc，将当前超参数组合及其对应的训练结果添加到 results 列表中，结果包括隐藏层大小、学习率、正则化系数、激活函数、学习率衰减率、最佳验证准确率、最终训练损失和最终验证损失。打印当前超参数组合的最佳验证准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf213c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        best_val_acc = max(history['accuracy'])\n",
    "        results.append({\n",
    "            'hidden_size': hidden_size,\n",
    "            'learning_rate': lr,\n",
    "            'reg_lambda': reg,\n",
    "            'activation': activation,\n",
    "            'learning_rate_decay': decay,\n",
    "            'val_acc': best_val_acc,\n",
    "            'final_train_loss': history['train_loss'][-1],\n",
    "            'final_val_loss': history['val_loss'][-1]\n",
    "        })\n",
    "\n",
    "        print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ee8b9f",
   "metadata": {},
   "source": [
    "根据验证集准确率找出表现最优的超参数组合及其结果，并打印搜索总结信息，包括测试的组合总数、前 3 个表现最优的超参数组合及其验证集准确率，以及最佳超参数组合和对应的验证集准确率。保存结果并返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9465c90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    if not results:\n",
    "        print(\"No valid results obtained.\")\n",
    "        return [], {}\n",
    "\n",
    "    best_result = max(results, key=lambda x: x['val_acc'])\n",
    "\n",
    "\n",
    "    print(\"\\n=== Search Summary ===\")\n",
    "    print(f\"Total combinations tested: {len(results)}\")\n",
    "    print(\"\\nTop 3 Performers:\")\n",
    "    for i, r in enumerate(sorted(results, key=lambda x: -x['val_acc'])[:3]):\n",
    "        print(f\"{i + 1}. val_acc={r['val_acc']:.4f} (hidden_size={r['hidden_size']}, \"\n",
    "              f\"lr={r['learning_rate']:.3f}, reg={r['reg_lambda']:.3f}, \"\n",
    "              f\"activation={r['activation']}, decay={r['learning_rate_decay']})\")\n",
    "\n",
    "    print(\"\\nBest Hyperparameters:\")\n",
    "    print(f\"Hidden Size: {best_result['hidden_size']}\")\n",
    "    print(f\"Learning Rate: {best_result['learning_rate']:.3f}\")\n",
    "    print(f\"Regularization: {best_result['reg_lambda']:.3f}\")\n",
    "    print(f\"Activation: {best_result['activation']}\")\n",
    "    print(f\"Learning Rate Decay: {best_result['learning_rate_decay']}\")\n",
    "    print(f\"Validation Accuracy: {best_result['val_acc']:.4f}\")\n",
    "\n",
    "    save_results(results, best_result)\n",
    "\n",
    "    return results, best_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bcb2a4",
   "metadata": {},
   "source": [
    "#### (2) 保存超参数搜索结果\n",
    "\n",
    "save_results函数将超参数调优的结果保存到指定目录下的 JSON 文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cf14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, best_result, save_dir='results'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    json_path = os.path.join(save_dir, 'hyperparam_results_3.json')\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'all_results': results,\n",
    "            'best_result': best_result\n",
    "        }, f, indent=2)\n",
    "\n",
    "    best_path = os.path.join(save_dir, 'best_result.json')\n",
    "    with open(best_path, 'w') as f:\n",
    "        json.dump(best_result, f, indent=2)\n",
    "\n",
    "    print(f\"\\nResults saved to:\")\n",
    "    print(f\"- JSON: {json_path}\")\n",
    "    print(f\"- Best result: {best_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470b10ae",
   "metadata": {},
   "source": [
    "### 6. 模型评估\n",
    "\n",
    "test.py：加载已训练的三层神经网络模型，并使用测试数据对其进行评估。计算并输出模型在测试集上的准确率，以此衡量模型的性能表现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb0212",
   "metadata": {},
   "source": [
    "evaluate函数根据测试数据的特征维度确定输入层大小input_size，通过测试标签的唯一类别数确定输出层大小output_size，从config模块中获取默认的隐藏层大小hidden_size。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb71a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_path, test_data, test_labels):\n",
    "    input_size = test_data.shape[1]\n",
    "    output_size = len(np.unique(test_labels))\n",
    "    hidden_size = config.DEFAULT_CONFIG['hidden_size']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a121c73",
   "metadata": {},
   "source": [
    "\n",
    "创建ThreeLayerNN模型实例，使用np.load函数加载已训练好的模型参数，该参数存储在指定路径中。将加载的权重和偏置参数分别赋值给模型的对应属性，完成模型的初始化。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d0d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model = ThreeLayerNN(input_size, hidden_size, output_size)\n",
    "    model_parameter = np.load(model_path, allow_pickle=True).item()\n",
    "\n",
    "    model.w1 = model_parameter['w1']\n",
    "    model.b1 = model_parameter['b1']\n",
    "    model.w2 = model_parameter['w2']\n",
    "    model.b2 = model_parameter['b2']\n",
    "    model.w3 = model_parameter['w3']\n",
    "    model.b3 = model_parameter['b3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd68d16",
   "metadata": {},
   "source": [
    "调用模型的accuracy方法，传入测试数据和测试标签，计算模型在测试集上的准确率。打印模型在测试集上的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cabb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    test_accuracy = model.accuracy(test_data, test_labels)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e184d",
   "metadata": {},
   "source": [
    "### 7. 可视化分析\n",
    "\n",
    "utils.py：提供了可视化工具函数，包括权重可视化和混淆矩阵绘制。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5237be0f",
   "metadata": {},
   "source": [
    "#### (1) 权重可视化\n",
    "\n",
    "visualize_weights函数用于可视化神经网络第一层权重的图像表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weights(weights, save_path=None):\n",
    "    w = weights['w1']\n",
    "    w = (w - w.min()) / (w.max() - w.min())\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(100):\n",
    "        plt.subplot(10, 10, i + 1)\n",
    "        plt.imshow(w[:, i].reshape(32, 32, 3))\n",
    "        plt.axis('off')\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd14eed9",
   "metadata": {},
   "source": [
    "#### (2) 混淆矩阵绘制\n",
    "\n",
    "plot_confusion_matrix函数用于绘制混淆矩阵，以直观评估分类模型的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad04ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, save_path=None):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175cd872",
   "metadata": {},
   "source": [
    "### 8. 默认配置参数\n",
    "\n",
    "config.py：定义了模型训练和调优过程中使用的默认配置参数，以及 CIFAR - 10 数据集的类别名称。可以根据需要修改这些参数，以调整模型的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d360c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CONFIG = {\n",
    "    'hidden_size': 512,\n",
    "    'activation': 'leaky_relu',\n",
    "    'num_epochs': 100,\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 0.018,\n",
    "    'reg_lambda': 0.011,\n",
    "    'learning_rate_decay': 0.98,\n",
    "    'val_ratio': 0.1,\n",
    "    'save_dir': 'results'\n",
    "}\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ead221",
   "metadata": {},
   "source": [
    "## 四、实验结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeab837",
   "metadata": {},
   "source": [
    "### (1)超参数调优结果\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
